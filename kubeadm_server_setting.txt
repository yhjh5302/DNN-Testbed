#### docker image build and push pull
docker build -t yhjh5302/node-web-app-rasp .
docker push yhjh5302/node-web-app-rasp
docker pull yhjh5302/node-web-app-rasp

##############################
#### for nvidia
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -

# install
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update

sudo apt-cache policy kubelet
sudo apt-cache policy kubeadm
sudo apt-cache policy kubectl

sudo apt-get install -y kubelet=1.22.3-00 kubeadm=1.22.3-00 kubectl=1.22.3-00 kubernetes-cni nvidia-container-runtime nvidia-docker2
#sudo apt-mark hold kubelet kubeadm kubectl kubernetes-cni
sudo systemctl enable --now kubelet


sudo apt-get install -y kubelet kubeadm kubectl kubernetes-cni
sudo apt install python3-pip nano
sudo -H pip3 install -U jetson-stats
sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target
sudo nano /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf


# docker with systemd
sudo docker info | grep -i cgroup
sudo systemctl edit docker.service
'''
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime --exec-opt native.cgroupdriver=systemd
'''
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo docker info | grep -i cgroup
'''
sudo nano /etc/docker/daemon.json
{
    "insecure-registries" : ["localhost:32000"] 
}
{
    "exec-opts" : ["native.cgroupdriver=systemd"]
    "log-driver" : "json-file",
    "log-opts" : {
        "max-size": "100m"
    },
    "storage-driver" : "overlay2"
}
{
    "data-root": "/mnt/docker-data",
    "storage-driver": "overlay2"
}
{
    "default-runtime" : "nvidia",
    "runtimes" : {
        "nvidia" : {
            "path" : "/usr/bin/nvidia-container-runtime",
            "runtimeArgs" : []
         }
    }
}
'''

# uninstall
sudo apt-mark unhold kubelet kubeadm kubectl kubernetes-cni
sudo apt-get remove -y --purge kubelet kubeadm kubectl kubernetes-cni
sudo apt-get autoremove -y

# run kubeadm
sudo swapoff -a
sudo sysctl -w net.bridge.bridge-nf-call-ip6tables=1
sudo sysctl -w net.bridge.bridge-nf-call-iptables=1
sudo sysctl -w net.ipv4.ip_forward=1
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
sudo kubeadm config images pull

#### mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#### (After kubeadm init and join) Add following options to /var/lib/kubelet/config.yaml for enabling swap memory
sudo nano /var/lib/kubelet/config.yaml
featureGates:
  NodeSwap: true
failSwapOn: false
memorySwap:
  swapBehavior: UnlimitedSwap

sudo mkdir -p /etc/containerd && containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
sudo nano /etc/containerd/config.toml
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = true

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
            privileged_without_host_devices = false
            runtime_engine = ""
            runtime_root = ""
            runtime_type = "io.containerd.runc.v1"

            [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
              BinaryName = "/usr/bin/nvidia-container-runtime"
              SystemdCgroup = true

sudo systemctl restart containerd

#### inspect
journalctl -fu kubelet

#### network
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
kubectl get nodes

#### GPU enable
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta4/nvidia-device-plugin.yml

#### master node untaint
kubectl taint nodes jin-tfx255 node-role.kubernetes.io/master-
kubectl taint nodes --all node-role.kubernetes.io/control-plane-

#### 
kubectl apply -f pipeline-deploy.yaml
kubectl apply -f service.yaml
kubectl delete -f pipeline-deploy.yaml
kubectl delete -f service.yaml
kubectl exec -it layer1-548fd6689c-69t6d bash

####
sudo kubeadm config images list
sudo kubeadm config images pull
sudo kubeadm config images pull --image-repository docker.io

#### cluster role binding (WARNING: This allows any user with read access to secrets or the ability to create a pod to access super-user credentials.)
kubectl create clusterrolebinding serviceaccounts-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts

#### dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml

#### getting dashboard key
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')

#### proxy
kubectl proxy
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/



#### slave wifi setting
sudo nano /etc/netplan/50-cloud-init.yaml
nmtui
hostname -I
sudo iw event
sudo iw wlan0 scan > scan_result
sudo iw reg set US

#### slave
sudo swapoff -a
sudo kubeadm join 192.168.0.2:6443 --token ffb2dv.1hkm9b8y7vqnwxls --discovery-token-ca-cert-hash sha256:f073c3eb91f54776cef89bf4ee7dc2650bae85d7d3dfc09feb4cbb3092e8dd3d

#### (NOT Necessary) nginx run on master node
kubectl apply -f capstone_2020/ingress-nginx-mandatory.yaml
kubectl apply -f capstone_2020/create-ingress.yaml
kubectl expose deploy nginx-ingress-controller --type=NodePort -n ingress-nginx
kubectl get svc -n ingress-nginx

kubectl delete deploy nginx-ingress-controller -n ingress-nginx



#### run node-web-app
kubectl create deployment node-web-app --image=yhjh5302/node-web-app-rasp

#### expose node-web-app service
kubectl expose deployment node-web-app --type=NodePort --name=my-service --port=8080
or
kubectl apply -f capstone_2020/service-expose.yaml

#### pods check
kubectl describe pods node-web-app

#### debug
kubectl exec -it --namespace prod node-web-app bash


#### end
kubectl get service
kubectl get deployment
kubectl get pod
kubectl get node

kubectl delete services my-service
kubectl delete deployment node-web-app

sudo kubeadm reset
sudo rm -rf /etc/cni/net.d



#### kubeflow
#### kustomize install
curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" >> test.sh; bash test.sh 4.5.7; rm test.sh; sudo mv kustomize /usr/local/bin/

#### persistent volume
sudo mkdir /mnt/pv{1..5}

kubectl create -f - <<EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: Immediate
EOF

kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml

kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

kubectl create -f - <<EOF
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume1
spec:
  storageClassName: local-path
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv1"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume2
spec:
  storageClassName: local-path
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv2"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume3
spec:
  storageClassName: local-path
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv3"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume4
spec:
  storageClassName: local-path
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv4"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume5
spec:
  storageClassName: local-path
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv5"
EOF
