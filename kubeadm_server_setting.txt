#### docker image build and push pull
docker build -t yhjh5302/node-web-app-rasp .
docker push yhjh5302/node-web-app-rasp
docker pull yhjh5302/node-web-app-rasp

##############################
#### for nvidia
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -

# install
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update

sudo apt-cache policy kubelet
sudo apt-cache policy kubeadm
sudo apt-cache policy kubectl

sudo apt-get install -y kubelet=1.22.3-00 kubeadm=1.22.3-00 kubectl=1.22.3-00 kubernetes-cni nvidia-container-runtime nvidia-docker2
#sudo apt-mark hold kubelet kubeadm kubectl kubernetes-cni
sudo systemctl enable --now kubelet


sudo apt-get install -y kubelet kubeadm kubectl kubernetes-cni
sudo apt install python3-pip nano
sudo -H pip3 install -U jetson-stats
sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target
sudo nano /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf


# docker with systemd
sudo docker info | grep -i cgroup
sudo systemctl edit docker.service
'''
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime --exec-opt native.cgroupdriver=systemd
'''
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo docker info | grep -i cgroup
'''
sudo nano /etc/docker/daemon.json
{
    "insecure-registries" : ["localhost:32000"] 
}
{
    "exec-opts" : ["native.cgroupdriver=systemd"]
    "log-driver" : "json-file",
    "log-opts" : {
        "max-size": "100m"
    },
    "storage-driver" : "overlay2"
}
{
    "data-root": "/mnt/docker-data",
    "storage-driver": "overlay2"
}
{
    "default-runtime" : "nvidia",
    "runtimes" : {
        "nvidia" : {
            "path" : "/usr/bin/nvidia-container-runtime",
            "runtimeArgs" : []
         }
    }
}
'''

# uninstall
sudo apt-mark unhold kubelet kubeadm kubectl kubernetes-cni
sudo apt-get remove -y --purge kubelet kubeadm kubectl kubernetes-cni
sudo apt-get autoremove -y

# run kubeadm
sudo swapoff -a
sudo sysctl -w net.bridge.bridge-nf-call-ip6tables=1
sudo sysctl -w net.bridge.bridge-nf-call-iptables=1
sudo sysctl -w net.ipv4.ip_forward=1
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
sudo kubeadm config images pull

#### mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#### Raspberry Pi vxlan
sudo apt install linux-modules-extra-5.15.0-1005-raspi linux-modules-extra-raspi


#### (After kubeadm init and join) Add following options to /var/lib/kubelet/config.yaml for enabling swap memory
sudo nano /var/lib/kubelet/config.yaml
featureGates:
  NodeSwap: true
failSwapOn: false
memorySwap:
  swapBehavior: UnlimitedSwap


#### ubuntu swap on
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
sudo nano /etc/fstab
/swapfile swap swap defaults 0 0


#### containerd runtime setting
sudo mkdir -p /etc/containerd && containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
sudo nano /etc/containerd/config.toml
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = true

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
            privileged_without_host_devices = false
            runtime_engine = ""
            runtime_root = ""
            runtime_type = "io.containerd.runc.v1"

            [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
              BinaryName = "/usr/bin/nvidia-container-runtime"
              SystemdCgroup = true

sudo systemctl restart containerd

#### inspect
journalctl -fu kubelet

#### network
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
kubectl get nodes

#### GPU enable
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta4/nvidia-device-plugin.yml

#### master node untaint
kubectl taint nodes jin-tfx255 node-role.kubernetes.io/master-
kubectl taint nodes --all node-role.kubernetes.io/control-plane-

#### 
kubectl apply -f pipeline-deploy.yaml
kubectl apply -f service.yaml
kubectl delete -f pipeline-deploy.yaml
kubectl delete -f service.yaml
kubectl exec -it layer1-548fd6689c-69t6d bash

####
sudo kubeadm config images list
sudo kubeadm config images pull
sudo kubeadm config images pull --image-repository docker.io

#### dashboard
kubectl apply -f dashboard.yaml



#### slave wifi setting
sudo nano /etc/netplan/50-cloud-init.yaml
nmtui
hostname -I
sudo iw event
sudo iw wlan0 scan > scan_result
sudo iw reg set US

#### slave
sudo swapoff -a
sudo kubeadm join 192.168.0.2:6443 --token ffb2dv.1hkm9b8y7vqnwxls --discovery-token-ca-cert-hash sha256:f073c3eb91f54776cef89bf4ee7dc2650bae85d7d3dfc09feb4cbb3092e8dd3d

#### (NOT Necessary) nginx run on master node
kubectl apply -f capstone_2020/ingress-nginx-mandatory.yaml
kubectl apply -f capstone_2020/create-ingress.yaml
kubectl expose deploy nginx-ingress-controller --type=NodePort -n ingress-nginx
kubectl get svc -n ingress-nginx

kubectl delete deploy nginx-ingress-controller -n ingress-nginx



#### run node-web-app
kubectl create deployment node-web-app --image=yhjh5302/node-web-app-rasp

#### expose node-web-app service
kubectl expose deployment node-web-app --type=NodePort --name=my-service --port=8080
or
kubectl apply -f capstone_2020/service-expose.yaml

#### pods check
kubectl describe pods node-web-app

#### debug
kubectl exec -it --namespace prod node-web-app bash


#### end
kubectl get service
kubectl get deployment
kubectl get pod
kubectl get node

kubectl delete services my-service
kubectl delete deployment node-web-app

sudo kubeadm reset
sudo rm -rf /etc/cni/net.d



#### image 관리 docker/ctr
sudo docker images --all
sudo docker build -t yhjh5302/kubeflow-test .
sudo docker run --rm -it --network=host --ipc=host yhjh5302/kubeflow-test
sudo docker ps | grep yhjh5302 | awk '{print $1 " bash"}' | xargs -o sudo docker exec -it
sudo docker rmi yhjh5302/kubeflow-test
sudo docker system prune --volumes
sudo ctr -n k8s.io image list
sudo ctr -n k8s.io images rm docker.io/yhjh5302/kubeflow-test:latest
sudo ctr -n k8s.io image list | grep pytorch | awk '{print $1}' | xargs sudo ctr -n k8s.io images rm



#### kustomize install
curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" >> test.sh; bash test.sh 4.5.7; rm test.sh; sudo mv kustomize /usr/local/bin/

#### kubeflow install
https://github.com/kubeflow/manifests/tree/v1.6-branch

#### persistent volume
sudo mkdir /mnt/pv{1..5}
sudo rm -rf /mnt/pv{1..5}/*



# manifest
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# 직접 선언
kubectl create -f - <<EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: Immediate
EOF

kubectl create -f - <<EOF
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume1
spec:
  storageClassName: local-path
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv1"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume2
spec:
  storageClassName: local-path
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv2"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume3
spec:
  storageClassName: local-path
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv3"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume4
spec:
  storageClassName: local-path
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/pv4"
EOF

#### Kubeflow authservice-0 permission denied 에러 해결 spec.template.spec.initContainers
kubectl edit statefulset -n istio-system authservice
      initContainers:
      - name: fix-permission
        image: busybox
        command: ['sh', '-c']
        args: ['chmod -R 777 /var/lib/authservice;']
        volumeMounts:
        - mountPath: /var/lib/authservice
          name: data

#### 오류: config.go:46] config=main.Config{CertFile:"/etc/webhook/certs/tls.crt", KeyFile:"/etc/webhook/certs/tls.key"} Error: too many open files
sudo sysctl fs.inotify.max_user_instances=1280
sudo sysctl fs.inotify.max_user_watches=655360

#### after kubeflow install (https://www.kubeflow.org/docs/distributions/ibm/deploy/authentication/)
kubectl patch svc istio-ingressgateway -n istio-system -p '{"spec":{"type":"NodePort"}}'

#### APP_SECURE_COOKIES -> false
kubectl -n kubeflow edit deployment jupyter-web-app-deployment
kubectl -n kubeflow edit deployment volumes-web-app-deployment
kubectl -n kubeflow edit deployment tensorboards-web-app-deployment

# nvidia cuda install (https://developer.nvidia.com/cuda-11-7-0-download-archive)
wget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda_11.7.0_515.43.04_linux.run
sudo sh cuda_11.7.0_515.43.04_linux.run

#### nvidia plugin
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.13.0/nvidia-device-plugin.yml
helm repo add nvdp https://nvidia.github.io/gpu-feature-discovery
helm repo update

# nvidia-device-plugin nvdp/nvgfd 통합 설치 none/single/mixed
helm upgrade -i nvdp nvdp/nvidia-device-plugin \
    --version=0.13.0 \
    --namespace nvidia-device-plugin \
    --create-namespace \
    --set compatWithCPUManager=true \
    --set migStrategy=single \
    --set gfd.enabled=true

# nvidia-device-plugin nvdp/nvgfd 별도 설치
helm install \
    --version=0.13.0 \
    --generate-name \
    --namespace nvidia-device-plugin \
    --create-namespace \
    --set migStrategy=mixed \
    nvdp/nvidia-device-plugin
helm install \
    --version=0.7.0 \
    --generate-name \
    --namespace nvidia-device-plugin \
    --create-namespace \
    --set migStrategy=mixed \
    nvgfd/gpu-feature-discovery

# nvdp 제거
helm -n nvidia-device-plugin ls | grep nvidia | awk '{print $1}' | xargs helm -n nvidia-device-plugin uninstall

# NVIDIA GPU status check
nvidia-smi

# GPU persistence mode on
nvidia-smi -pm 1

# MIG enable
nvidia-smi -mig 1

# reset GPUs (만약 위 enable이 waring이 뜨거나 에러가 나서 Disable/Enabled* 상태에 머물경우 실행, 그래도 안되면 프로세스 직접 찾아서 꺼야함)
nvidia-smi --gpu-reset -i 0, 1, 2, 3

# MIG status check
nvidia-smi -L

# MIG gpu instance profile list check
nvidia-smi mig -lgip

# create gpu instance (19 - 1g.3gb / 14 - 2g.6gb / 0 - 4g.24gb)
nvidia-smi mig -cgi 19,19,19,19
nvidia-smi mig -cgi 14,14
nvidia-smi mig -cgi 0

# MIG compute instance profile list check
nvidia-smi mig -lcip

# create compute instance (0 - 1g.3gb / 1 - 2g.6gb / 3 - 4g.24gb)
nvidia-smi mig -cci 0,0,0,0
nvidia-smi mig -cci 1,1
nvidia-smi mig -cci 3

# delete compute instance
nvidia-smi mig -dci

# delete gpu instance
nvidia-smi mig -dgi

# nvidia-persistenced 데몬 설정
git clone https://github.com/NVIDIA/nvidia-persistenced
cd nvidia-persistenced/init/
sh install.sh

# nvidia-persistenced 체크
systemctl status nvidia-persistenced

# MIG 리부팅시 설정
git clone https://github.com/NVIDIA/mig-parted
cd mig-parted/deployments/systemd/
sh install.sh

# MIG-parted config 수정 (1g.3gb는 기본 프로필상에 존재하지 않아 추가가 필요함.)
nano /etc/nvidia-mig-manager/config.yaml
-> 맨 아래 config 추가 (indentation 틀릴 경우 에러 발생)
  all-1g.3gb:
    - devices: all
      mig-enabled: true
      mig-devices:
        "1g.3gb": 4

# MIG 상태 저장
nvidia-mig-parted apply -f /etc/nvidia-mig-manager/config.yaml -k /etc/nvidia-mig-manager/hooks.yaml -c all-1g.3gb



#### user add
kubectl create -f - <<EOF
apiVersion: kubeflow.org/v1beta1
kind: Profile
metadata:
  name: profileName   # replace with the name of profile you want, this will be user's namespace name
spec:
  owner:
    kind: User
    name: userid@email.com   # replace with the email of the user
  resourceQuotaSpec:    # resource quota can be set optionally
    hard:
      cpu: "16"
      memory: 32Gi
      # requests.nvidia.com/gpu: "1"
      persistentvolumeclaims: "10"
      requests.storage: "32Gi"
EOF
kubectl edit cm dex -n auth
    - email: user@example.com
      hash: $2y$12$4K/VkmDd1q1Orb3xAt82zu8gk7Ad6ReFR4LCP9UeYE90NLiN9Df72
      # https://github.com/dexidp/dex/pull/1601/commits
      # FIXME: Use hashFromEnv instead
      username: user
      userID: "15841185641784"
python3 -c 'import bcrypt; print(bcrypt.hashpw(b"password", bcrypt.gensalt(rounds=10)).decode("ascii"))'
kubectl rollout restart deployment dex -n auth



#### 노트북 외부 ssh 연결
# istio-ingressgateway통해서 통신할때 사용. Gateway
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: test-gateway
  namespace: yhjin
spec:
  selector:
    istio: ingressgateway
  servers:
  - hosts:
    - "*"
    port:
      name: test-ssh
      number: 22
      protocol: TCP
--- # istio-ingressgateway통해서 통신할때 사용. virtual service
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: test-ssh
  namespace: yhjin
spec:
  hosts:
  - '*'
  gateways:
  - yhjin/test-gateway
  tcp:
  - match:
    - port: 22
    route:
    - destination:
        host: test.yhjin.svc.cluster.local
        port:
          number: 22
--- # 포트 authorization 허용 (네임스페이스 전체 적용)
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: ssh-policy
  namespace: yhjin
spec:
  action: ALLOW
  rules:
  - to:
    - operation:
        ports: ["22"]
--- # 서비스 선언 - 새로 추가해서 노드포트 개방
apiVersion: v1
kind: Service
metadata:
  name: test-ssh
  namespace: yhjin
spec:
  type: NodePort
  ports:
  - name: test-ssh
    targetPort: 22
    port: 22
    nodePort: 30001
    protocol: TCP
  selector:
    statefulset: test
--- # 서비스 선언 - 있던거에 포트만 추가
kubectl -n kubeflow edit gw kubeflow-gateway
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: kubeflow-gateway
  namespace: kubeflow
spec:
  selector:
    app: istio-ingressgateway
  servers:                   <- 뒷부분 추가
  - hosts:
    - '*'
    port:
      number: 22
      name: ssh
      protocol: TCP

#### RedHat8 nvidia driver update
sudo su
wget https://us.download.nvidia.com/tesla/470.57.02/nvidia-driver-local-repo-rhel8-470.57.02-1.0-1.x86_64.rpm
dnf remove cuda-drivers
rpm -i nvidia-driver-local-repo-rhel8-470.57.02-1.0-1.x86_64.rpm
dnf install cuda-drivers
rm nvidia-driver-local-repo-rhel8-470.57.02-1.0-1.x86_64.rpm